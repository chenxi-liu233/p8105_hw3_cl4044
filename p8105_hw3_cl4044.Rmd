---
title: "p8105_hw3_cl4044"
author: "Chenxi Liu"
date: "10/6/2020"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(patchwork)
library(p8105.datasets)
knitr::opts_chunk$set(
	fig.width = 6, 
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1
```{r}
data("instacart")
```
This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. 

Observations are the level of items in orders by user. There are user / order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes. 

How many aisles, and which are most items from?

There are  `r nlevels(pull(instacart,aisle))` aisles. Fresh Vegetables are the most items from.
```{r}
instacart %>% 
	count(aisle) %>% 
	filter(n > 10000) %>% 
	mutate(
		aisle = factor(aisle),
		aisle = fct_reorder(aisle, n)
	) %>% 
	ggplot(aes(x = aisle, y = n)) + 
	geom_point() + 
	theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


Let's make a table!!

```{r}
instacart %>% 
	filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
	group_by(aisle) %>% 
	count(product_name) %>% 
	mutate(rank = min_rank(desc(n))) %>% 
	filter(rank < 4) %>% 
	arrange(aisle, rank) %>% 
	knitr::kable()
```


Apples vs ice cream..

```{r}
instacart %>% 
	filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
	group_by(product_name, order_dow) %>% 
	summarize(mean_hour = mean(order_hour_of_day)) %>% 
	pivot_wider(
		names_from = order_dow,
		values_from = mean_hour
	)
```

## Problem 2

Load, tidy, and wrangle the data.
```{r}
accel_df = read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>%
  pivot_longer(cols = starts_with("activity"), 
               names_to = "minute", 
               names_prefix = "activity_", 
               values_to = "activity_count"
               ) %>%
  mutate(weekend = if_else(day %in% c("Saturday", "Sunday"), TRUE, FALSE)) %>%
  mutate(day = as.factor(day),
         minute = as.numeric(minute),
         week = as.integer(week),
         day_id = as.integer(day_id)
         )
accel_df
  
```

This dataset has `r nrow(accel_df)` observations. The dataset contains 6 variables:
\
`week`: the week of the obeservation, a integer variable ranging from  `r min(accel_df$week)` to `r max(accel_df$week)`. 
\
`day_id`: the unique id of the day of the observation, a integer varible ranging from `r min(accel_df$day_id)` to `r max(accel_df$day_id)`
\
`day`: the name of the day of the week, a factor variable from Monday to Sunday.
\
`activity_count`: per-minute activity counts, a double variable ranging from `r min(accel_df$activity_count)` to `r max(accel_df$activity_count)`. 
\
`weekend`: a logical variable indicates whether the day is a day of observation is a weekend.

create a table showing the aggregation accross minutes to create a total activity variable for each day
```{r}
table_df =
  accel_df %>% 
  group_by(week, day) %>% 
  summarize(day_activity_sum = sum(activity_count)) %>%
  pivot_wider(
      id_cols = "week",
      names_from = "day",
      values_from = "day_activity_sum"
    ) %>%
  select(week, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday) %>%
  knitr::kable(caption = "Aggregated activity counts by day and week")
table_df
```

The weekday activities counts are fairly consistant throughout the week. From week 1 to week 3, the activity counts on weekends are slightly lower than that of on weekdays. However, form week 4 to week 5, the activity counts on weekends are much lower than that of on weekdays. 

```{r, fig.width=9,fig.height=18}
accel_df %>%
  group_by(day, minute) %>% 
  summarize(activity_sum = sum(activity_count)) %>% 
  ggplot(aes(x = minute ,y = activity_sum, color = day)) + 
  geom_point(alpha = .2, size = 0.5) +
  geom_smooth(size = 0.8) + 
  theme(legend.position = "bottom") +
  labs(
    title = "24-hour Activity Time Courses Across Each Day",
    x = "Hours",
    y = "Activity Counts",
    caption = "Accelerometer data collected on a 63 year-old male with BMI 25",
    color = "Day of the week") +
  scale_x_continuous(
    breaks = c(seq(0, 1440, by = 60)),
    labels = c(seq(0, 24, by = 1))
  )
```

From the graph above, I observed that this person usually sleep from 23PM to 5AM everyday because the activity counts during these hours are relatively low. This person also usually has the highest activity count on Friday nights. 

## Problem 3
```{r}
data("ny_noaa")
ny_df = ny_noaa %>%
  separate(date, into = c("year","month","day"),sep = "-") %>% 
  mutate(
    year = as.integer(year),
    month = as.integer(month),
    month = (month.name[month]),
    day = as.integer(day),
    prcp = as.double(prcp),
    prcp = prcp / 10,
    tmax = as.numeric(tmax),
    tmin = as.numeric(tmin),
    tmax = tmax / 10,
    tmin = tmin / 10
  ) 
ny_df %>%
  group_by(snow) %>%
  drop_na(snow) %>%
  summarize(count = n()) %>%
  arrange(desc(count))
```
For snowfall, the top five most commonly observed snowfall values are 0, 25, 13, 51 and 76. Since there is no snow most of days in a year, 0 should be the most commonly observed snowfall value. 

```{r}
ny_df %>%
  filter(month == c("January", "July")) %>%
  group_by(id, year, month) %>%
  summarize(mean_max = mean(tmax, na.rm = TRUE)) %>%
  ggplot(aes(x = year, y = mean_max)) +
  geom_point(aes(color = id, alpha = 0.01))+ 
    labs(
    title = "Max tempretures in NY weather stations in January and July", 
    y = "Tempreture (°C)", 
    x = "Year",
    caption = "Data from ny_noaa package") +
  theme(legend.position = "none") +
  facet_grid(.~month)
```

Most of the station's average max temperature are between -10°C and 10°C in January; between 20°C and 35°C in July. The temperatures in January varies more than that in July. There are also a few outliers observed. 


```{r, fig.width=6,fig.height=8}
tmax_tmin_plot =
  ny_df %>%
  ggplot(aes(x = tmax, y = tmin)) +
  geom_hex() +
    labs(
    title = "Max vs. Min Temperatures in NY",
    x = "Max Temperature (°C)",
    y = "Min Temperature (°C)",
    caption = "Data from ny_noaa package"
  ) +
  theme(legend.position = "right")
```

```{r}
snowfall_plot = 
  ny_df %>%
  drop_na() %>% 
  filter(snow > 0 & snow < 100) %>%
  ggplot(aes(x = year, y = snow, group = year, fill = year)) +
  geom_violin() +
  stat_summary(mult=1, geom="pointrange", size = 0.1, color="red")
    labs(
    title = "Distribution of Snowfall(mm)",
    x = "Year",
    y = "Snowfall (mm)",
    caption = "Data from ny_noaa package"
  ) +
  scale_x_continuous(
    breaks = c(seq(1980, 2010, by = 5))
  ) +
  theme(legend.position = "none")
```

Two-panel plot using patchwork
```{r, fig.width=7,fig.height=14}
tmax_tmin_plot + snowfall_plot
```

In the Max vs. Min Temperatures in NY plot, the observations are mostly clustered on two spot, the (28, 14) and (10,0) cluster. Most of observations are found in the greenish hexes, all the purple hexes means the number observation is small (<10000). In the Snowfall Distribution plot, we can observe that the snowfall distributions across years looks similar to each other. So the snowfall each year is pretty consistent with an averafe of snowfall around 30mm. 

The `ny_df` dataset has `r nrow(ny_df)` rows and `r ncol(ny_df)` columns. After tidying the original dataset, the `ny_df` dataframe contains `9` variables: 
\
`id`: Weather station ID, a character variable. 
\
`year`: Year of observation, an integer variable, ranging from `r min(ny_df$year)` to `r max(ny_df$year)`.
\
`month`: The month of observation, a character variable. 
\
`day`: The day of observation, an integer variable, ranging from `r min(ny_df$day)` to `r max(ny_df$day)`.
\
`prcp`: Precipitation (mm), a double variable, ranging from `r min(ny_df$prcp, na.rm = TRUE)` to `r max(ny_df$prcp, na.rm = TRUE)`.
\
`snow`: Snowfall (mm), an integer variable, ranging from `r min(ny_df$snow, na.rm = TRUE)` to `r max(ny_df$snow, na.rm = TRUE)`.
\
`snwd`: Snow depth (mm), an integer variable, ranging from `r min(ny_df$snwd, na.rm = TRUE)` to `r max(ny_df$snwd, na.rm = TRUE)`.
\
`tmax`: Maximum temperature (C), a double variable, ranging from `r min(ny_df$tmax, na.rm = TRUE)` to `r max(ny_df$tmax, na.rm = TRUE)`.
\
`tmin`: Minimum temperature (C),  a double variable, ranging from `r min(ny_df$tmin, na.rm = TRUE)` to `r max(ny_df$tmin, na.rm = TRUE)`.

There is `r sum(is.na(ny_df))` counts of null value in `ny_df`. 
\
The total null values in each columns:
```{r}
colSums(is.na(ny_df))
```

Since our data frame is so large(`r nrow(ny_df)` X `r ncol(ny_df)`), the `r sum(is.na(ny_df))` counts of null value  does not matter very much. Our plots ignored these null values anyway. 